{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 20:08:31.352024: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-20 20:08:31.353284: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-20 20:08:31.376797: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-20 20:08:31.377423: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-20 20:08:31.861571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from scipy.ndimage import zoom\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing function\n",
    "def resize_images(images):\n",
    "    return np.array([zoom(image, 0.5) for image in images])\n",
    "\n",
    "# Resize\n",
    "x_train = resize_images(x_train)\n",
    "x_test = resize_images(x_test)\n",
    "\n",
    "# Then reshape\n",
    "x_train = x_train.reshape(60000, 14*14)\n",
    "x_test = x_test.reshape(10000, 14*14)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# normalize to range [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(14*14,)),\n",
    "    keras.layers.Dense(10, activation='relu'), \n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 1s 797us/step - loss: 0.8706 - accuracy: 0.7485 - val_loss: 0.4303 - val_accuracy: 0.8820\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 1s 756us/step - loss: 0.4043 - accuracy: 0.8854 - val_loss: 0.3465 - val_accuracy: 0.9028\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 1s 778us/step - loss: 0.3542 - accuracy: 0.8983 - val_loss: 0.3215 - val_accuracy: 0.9094\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 1s 811us/step - loss: 0.3307 - accuracy: 0.9052 - val_loss: 0.3048 - val_accuracy: 0.9131\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 1s 855us/step - loss: 0.3158 - accuracy: 0.9087 - val_loss: 0.2964 - val_accuracy: 0.9152\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 1s 903us/step - loss: 0.3057 - accuracy: 0.9128 - val_loss: 0.2903 - val_accuracy: 0.9179\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 1s 847us/step - loss: 0.2979 - accuracy: 0.9147 - val_loss: 0.2851 - val_accuracy: 0.9196\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 1s 924us/step - loss: 0.2922 - accuracy: 0.9166 - val_loss: 0.2807 - val_accuracy: 0.9199\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 0.2869 - accuracy: 0.9181 - val_loss: 0.2781 - val_accuracy: 0.9209\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 1s 759us/step - loss: 0.2817 - accuracy: 0.9200 - val_loss: 0.2709 - val_accuracy: 0.9232\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLa  (None, 196)               3         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " quant_dense (QuantizeWrapp  (None, 10)                1975      \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_dense_1 (QuantizeWra  (None, 10)                115       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2093 (8.18 KB)\n",
      "Trainable params: 2080 (8.12 KB)\n",
      "Non-trainable params: 13 (52.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Apply quantization to the layers\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# 'quantize_model' requires a recompile\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2806 - accuracy: 0.9190 - val_loss: 0.2676 - val_accuracy: 0.9240\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2747 - accuracy: 0.9216 - val_loss: 0.2649 - val_accuracy: 0.9248\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 1s 936us/step - loss: 0.2710 - accuracy: 0.9220 - val_loss: 0.2641 - val_accuracy: 0.9257\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 1s 945us/step - loss: 0.2677 - accuracy: 0.9234 - val_loss: 0.2692 - val_accuracy: 0.9227\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 1s 975us/step - loss: 0.2654 - accuracy: 0.9233 - val_loss: 0.2594 - val_accuracy: 0.9271\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 1s 905us/step - loss: 0.2625 - accuracy: 0.9245 - val_loss: 0.2589 - val_accuracy: 0.9274\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2603 - accuracy: 0.9261 - val_loss: 0.2578 - val_accuracy: 0.9262\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2582 - accuracy: 0.9257 - val_loss: 0.2537 - val_accuracy: 0.9279\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 1s 949us/step - loss: 0.2558 - accuracy: 0.9271 - val_loss: 0.2526 - val_accuracy: 0.9302\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 1s 821us/step - loss: 0.2540 - accuracy: 0.9270 - val_loss: 0.2514 - val_accuracy: 0.9280\n",
      "Test loss: 0.25918030738830566\n",
      "Test accuracy: 0.9286999702453613\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "history = q_aware_model.fit(x_train, y_train,\n",
    "                            epochs=epochs,\n",
    "                            validation_split=0.2)\n",
    "\n",
    "scores, acc = q_aware_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmps_yk188x/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmps_yk188x/assets\n",
      "/home/novalunosis/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-07-20 20:09:03.576285: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-07-20 20:09:03.576302: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-07-20 20:09:03.576524: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmps_yk188x\n",
      "2023-07-20 20:09:03.577472: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-07-20 20:09:03.577480: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmps_yk188x\n",
      "2023-07-20 20:09:03.580117: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-07-20 20:09:03.580860: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-07-20 20:09:03.610052: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmps_yk188x\n",
      "2023-07-20 20:09:03.623897: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 47373 microseconds.\n",
      "2023-07-20 20:09:03.633110: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4304"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "\n",
    "# Indicate that you want to perform default optimizations,\n",
    "# which include quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Define a generator function that provides your test data's numpy arrays\n",
    "def representative_data_gen():\n",
    "  for i in range(500):\n",
    "    yield [x_test[i:i+1]]\n",
    "\n",
    "# Use the generator function to guide the quantization process\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Set the input and output tensors to int8\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"q_aware_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"q_aware_model.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-128 -128    9   -9 -128 -128 -128 -128 -128 -128]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the input value to int8\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(x_test[0:1], dtype=np.int8)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Perform the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the result\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, _), (x_test_image, y_test_label) = mnist.load_data()\n",
    "\n",
    "# Resize and Normalize x_test_image to int8\n",
    "x_test_image = resize_images(x_test_image)\n",
    "x_test_image_norm = (x_test_image / 255.0 * 255 - 128).astype(np.int8)\n",
    "\n",
    "# Initialize an array to store the predictions\n",
    "predictions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the test data and make predictions\n",
    "for i in range(len(x_test_image_norm)):\n",
    "    test_image = np.expand_dims(x_test_image_norm[i].flatten(), axis=0)\n",
    "    \n",
    "    # Set the value for the input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "    \n",
    "    # Run the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions.append(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"q_aware_model.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object with all tensors \n",
    "#(an input + all weights and biases)\n",
    "tensors = {\n",
    "    \"input\": x_test_image[0].flatten(),\n",
    "    \"fc1_weights\": interpreter.get_tensor(1), \n",
    "    \"fc1_bias\": interpreter.get_tensor(2), \n",
    "    \"fc2_weights\": interpreter.get_tensor(4), \n",
    "    \"fc2_bias\": interpreter.get_tensor(5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('src/generated', exist_ok=True)\n",
    "\n",
    "for tensor_name, tensor in tensors.items():\n",
    "    with open(os.path.join('src', 'generated', f\"{tensor_name}.cairo\"), \"w\") as f:\n",
    "        f.write(\n",
    "            \"use array::ArrayTrait;\\n\" +\n",
    "            \"use orion::operators::tensor::core::{TensorTrait, Tensor, ExtraParams};\\n\" +\n",
    "            \"use orion::operators::tensor::implementations::impl_tensor_i32::Tensor_i32;\\n\" +\n",
    "            \"use orion::numbers::fixed_point::core::FixedImpl;\\n\" +\n",
    "            \"use orion::numbers::signed_integer::i32::i32;\\n\\n\" +\n",
    "            \"fn {0}() -> Tensor<i32> \".format(tensor_name) + \"{\\n\" +\n",
    "            \"    let mut shape = ArrayTrait::<usize>::new();\\n\"\n",
    "        )\n",
    "        for dim in tensor.shape:\n",
    "            f.write(\"    shape.append({0});\\n\".format(dim))\n",
    "        f.write(\n",
    "            \"    let mut data = ArrayTrait::<i32>::new();\\n\"\n",
    "        )\n",
    "        for val in np.nditer(tensor.flatten()):\n",
    "            f.write(\"    data.append(i32 {{ mag: {0}, sign: {1} }});\\n\".format(abs(int(val)), str(val < 0).lower()))\n",
    "        f.write(\n",
    "            \"let extra = ExtraParams { fixed_point: Option::Some(FixedImpl::FP16x16(())) }; \\n\" +\n",
    "            \"    TensorTrait::new(shape.span(), data.span(), Option::Some(extra))\\n\" +\n",
    "            \"}\\n\"\n",
    "        )\n",
    "      \n",
    "with open(os.path.join('src', 'generated.cairo'), 'w') as f:\n",
    "    for param_name in tensors.keys():\n",
    "        f.write(f\"mod {param_name};\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
